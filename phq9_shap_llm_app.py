"""
phq9_streamlit_app.py
======================

ì´ Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ì˜ì–‘ ì„­ì·¨ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš°ìš¸ì¦ ìœ„í—˜ë„ë¥¼ ì¶”ì •í•˜ê³ ,
ëª¨ë¸ì´ íŒë‹¨í•œ í•µì‹¬ ì˜í–¥ ìš”ì¸ì„ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ ê°„ë‹¨í•œ
ìì—°ì–´ ì„¤ëª…ê³¼ ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ì—¬ ì¶”ê°€ì ì¸ ê¶ê¸ˆì¦ì— ë‹µí•˜ë„ë¡
ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸(`phq9_shap_llm_app.py`)ëŠ” ì»¤ë§¨ë“œë¼ì¸
ìš©ìœ¼ë¡œ ì‘ì„±ë˜ì–´ ìˆìœ¼ë©° `shap`ì™€ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì˜ì¡´í•©ë‹ˆë‹¤.
ì´ ì•±ì€ ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ë¡œ ê²½ëŸ‰í™”ì™€ ìµœì í™”ì— ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤:

* ì™¸ë¶€ ë„¤íŠ¸ì›Œí¬ ì ‘ì† ì—†ì´ ì‹¤í–‰ë˜ë„ë¡ í•˜ê¸° ìœ„í•´ `shap`ê³¼ ëŒ€í˜• LLM
  ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ì œê±°í•˜ê±°ë‚˜ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
* Streamlitì˜ ìºì‹± ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ëª¨ë¸ê³¼ ê¸°íƒ€ ë¦¬ì†ŒìŠ¤ë¥¼ í•œ ë²ˆë§Œ
  ë¡œë“œí•˜ë„ë¡ í•˜ì—¬ ë°˜ë³µ ì‹¤í–‰ ì‹œ ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
* ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ SHAP ëŒ€ì‹  scikitâ€‘learnì˜ `coef_` í˜¹ì€
  `feature_importances_` ì†ì„±ì„ ì´ìš©í•´ ê·¼ì‚¬í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´
  ì˜ì¡´ì„±ì„ ì¤„ì´ê³  ê³„ì‚°ì„ ë‹¨ìˆœí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* ì‚¬ìš©ì ì…ë ¥ì„ ì›¹ ì–‘ì‹ìœ¼ë¡œ ë°›ê³  ê²°ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•˜ì—¬
  ì‚¬ìš©ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
* ì¶”ê°€ ì§ˆë¬¸ì„ ì…ë ¥í•  ìˆ˜ ìˆëŠ” ì±—ë´‡ ì˜ì—­ì„ ì œê³µí•˜ì§€ë§Œ, ì˜ë£Œì 
  ì¡°ì–¸ì´ ì•„ë‹Œ ì¼ë°˜ì ì¸ ì •ë³´ë§Œì„ ì œê³µí•©ë‹ˆë‹¤. ì±—ë´‡ ì‘ë‹µì€ ê°„ë‹¨í•œ
  ê·œì¹™ ê¸°ë°˜ í˜¹ì€ ì‘ì€ LLM(ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)ì„ ì‚¬ìš©í•´ ìƒì„±ë©ë‹ˆë‹¤.

ì£¼ì˜: ì´ ì•±ì€ êµìœ¡ì  ëª©ì ê³¼ ìê¸° ì´í•´ë¥¼ ë•ê¸° ìœ„í•œ ì°¸ê³ ìš©ì…ë‹ˆë‹¤.
ì •í™•í•œ ì§„ë‹¨ì´ë‚˜ ì¹˜ë£Œë¥¼ ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ ì •ì‹ ê±´ê°• ì „ë¬¸ê°€ì™€ ìƒë‹´í•´ì•¼ í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import json
import os
from typing import Dict, List, Optional

import numpy as np
import pandas as pd

import streamlit as st

try:
    # transformersëŠ” ì„ íƒì ì´ë©° ì¸í„°ë„· ì—°ê²°ì´ ì—†ìœ¼ë©´ ë¡œë“œì— ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # type: ignore
    _transformers_available = True
except Exception:
    _transformers_available = False


def disclaimer() -> None:
    """ì‚¬ìš©ìì—ê²Œ ì¤‘ìš”í•œ ë©´ì±…ë¬¸êµ¬ë¥¼ ë³´ì—¬ì¤€ë‹¤."""
    st.warning(
        """ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ê±´ê°• ê´€ë ¨ ì •ë³´ë¥¼ ì°¸ê³ ìš©ìœ¼ë¡œ ì œê³µí•˜ë©°,
        ì „ë¬¸ì ì¸ ì§„ë‹¨ì´ë‚˜ ì¹˜ë£Œë¥¼ ëŒ€ì‹ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìš°ìš¸ì¦ ìœ„í—˜ë„
        ì˜ˆì¸¡ ê²°ê³¼ëŠ” êµìœ¡ì  ìš©ë„ë¡œë§Œ ì‚¬ìš©í•´ì•¼ í•˜ë©°, ìì‹ ì˜ ì •ì‹ ê±´ê°•ì—
        ê´€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì˜ë£Œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.""",
        icon="âš ï¸",
    )


@st.cache_resource(show_spinner=False)
def load_model(model_path: str = "phq9_nutrition_model.pkl"):
    """
    Pickle íŒŒì¼ë¡œ ì €ì¥ëœ scikitâ€‘learn ëª¨ë¸ì„ ë¡œë“œí•œë‹¤.

    Parameters
    ----------
    model_path : str, optional
        ëª¨ë¸ íŒŒì¼ ê²½ë¡œ. ê¸°ë³¸ê°’ì€ `phq9_nutrition_model.pkl`ì´ë‹¤.

    Returns
    -------
    object
        joblibìœ¼ë¡œ ë¡œë“œëœ ëª¨ë¸ ê°ì²´. íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¡œë”©ì— ì‹¤íŒ¨í•˜ë©´
        ``None``ì„ ë°˜í™˜í•œë‹¤.
    """
    if not os.path.exists(model_path):
        st.error(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return None
    try:
        import joblib  # ë¡œì»¬ì— ì¡´ì¬í•˜ëŠ” ê²½ëŸ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬
    except ImportError:
        st.error("joblib ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ì— joblibì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    try:
        return joblib.load(model_path)
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None


@st.cache_resource(show_spinner=False)
def load_text_generator(model_name: str = "sshleifer/tiny-gpt2", device: int = -1):
    """
    ê²½ëŸ‰í™”ëœ í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ì„ ë¡œë“œí•œë‹¤. ì¸í„°ë„·ì´ ì°¨ë‹¨ë˜ì–´ ìˆê±°ë‚˜
    transformersê°€ ì—†ëŠ” ê²½ìš° ``None``ì„ ë°˜í™˜í•œë‹¤.

    Parameters
    ----------
    model_name : str, optional
        HuggingFace ëª¨ë¸ ì´ë¦„. ê¸°ë³¸ê°’ì€ ì‘ì€ GPTâ€‘2 ëª¨ë¸ì´ë‹¤.
    device : int, optional
        -1ì€ CPUë¥¼ ì˜ë¯¸í•œë‹¤. í™˜ê²½ì— GPUê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì¥ì¹˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆë‹¤.

    Returns
    -------
    Optional[callable]
        transformers.pipeline ê°ì²´ ë˜ëŠ” ``None``
    """
    if not _transformers_available:
        return None
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        gen = pipeline("text-generation", model=model, tokenizer=tokenizer, device=device)
        return gen
    except Exception:
        # ë¡œë”© ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
        return None


def compute_contributions(model: object, X: pd.DataFrame) -> np.ndarray:
    """
    SHAP ì—†ì´ ê° íŠ¹ì„±ì˜ ê¸°ì—¬ë„ë¥¼ ê·¼ì‚¬í•˜ì—¬ ë°˜í™˜í•œë‹¤. ëª¨ë¸ì— ë”°ë¼
    ``coef_``(ì„ í˜• ëª¨ë¸) ë˜ëŠ” ``feature_importances_``(íŠ¸ë¦¬ ëª¨ë¸)ë¥¼ ì‚¬ìš©í•œë‹¤.

    Parameters
    ----------
    model : object
        scikitâ€‘learn ë¶„ë¥˜ ëª¨ë¸.
    X : pandas.DataFrame
        ë‹¨ì¼ ìƒ˜í”Œì„ í¬í•¨í•˜ëŠ” DataFrame.

    Returns
    -------
    numpy.ndarray
        íŠ¹ì„±ë³„ ê¸°ì—¬ë„ ë²¡í„°. ê¸¸ì´ëŠ” íŠ¹ì„± ê°œìˆ˜ì™€ ê°™ë‹¤.
    """
    n_features = X.shape[1]
    # ê¸°ë³¸ê°’ì€ 0 ê¸°ì—¬ë„
    contributions = np.zeros(n_features)
    try:
        if hasattr(model, "coef_"):
            # ì„ í˜• ëª¨ë¸: coef_ í¬ê¸° (n_classes, n_features)
            # ì–‘ì„± í´ë˜ìŠ¤(ì¸ë±ìŠ¤ 0)ì„ ì‚¬ìš©í•˜ì—¬ ê°’ ê³±ì…ˆ
            coef = model.coef_[0]
            contributions = coef * X.iloc[0].values
        elif hasattr(model, "feature_importances_"):
            importances = model.feature_importances_
            contributions = importances * X.iloc[0].values
    except Exception:
        # ì˜ˆì™¸ ë°œìƒ ì‹œ 0 ê¸°ì—¬ë„ë¥¼ ìœ ì§€
        pass
    return contributions


def get_top_features(
    contributions: np.ndarray, feature_names: List[str], top_n: int = 5
) -> List[Dict[str, object]]:
    """
    ê¸°ì—¬ë„ ë²¡í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ì˜í–¥ë ¥ì´ í° íŠ¹ì„± ëª©ë¡ì„ ë°˜í™˜í•œë‹¤.

    Parameters
    ----------
    contributions : numpy.ndarray
        ê° íŠ¹ì„±ì˜ ê¸°ì—¬ë„ ë²¡í„°.
    feature_names : List[str]
        íŠ¹ì„± ì´ë¦„ ëª©ë¡.
    top_n : int, optional
        ë°˜í™˜í•  ìƒìœ„ íŠ¹ì„± ê°œìˆ˜. ê¸°ë³¸ê°’ì€ 5ì´ë‹¤.

    Returns
    -------
    List[Dict[str, object]]
        ``feature``, ``contribution``, ``direction`` í‚¤ë¥¼ ê°€ì§„ dict ëª©ë¡.
    """
    # ì ˆëŒ€ê°’ ê¸°ì¤€ ì •ë ¬
    indices = np.argsort(np.abs(contributions))[::-1][: min(top_n, len(contributions))]
    results: List[Dict[str, object]] = []
    for idx in indices:
        val = float(contributions[idx])
        direction = "increase" if val > 0 else "decrease"
        results.append({"feature": feature_names[idx], "contribution": abs(val), "direction": direction})
    return results


def build_explanation(
    probability: float,
    top_features: List[Dict[str, object]],
    generator: Optional[callable] = None,
) -> str:
    """
    ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì£¼ìš” íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ ì„¤ëª…ë¬¸ì„ ìƒì„±í•œë‹¤.
    ê°€ëŠ¥í•œ ê²½ìš° ê²½ëŸ‰ LLMì„ ì‚¬ìš©í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•œë‹¤.

    Parameters
    ----------
    probability : float
        ê¸ì • í´ë˜ìŠ¤(ìš°ìš¸ì¦ ìœ„í—˜) í™•ë¥ .
    top_features : List[Dict[str, object]]
        ê°€ì¥ ì˜í–¥ë ¥ì´ í° íŠ¹ì„± ëª©ë¡.
    generator : Optional[callable], optional
        transformers.pipeline ê°ì²´. Noneì´ë©´ ê·œì¹™ ê¸°ë°˜ ì„¤ëª…ì„ ì‚¬ìš©í•œë‹¤.

    Returns
    -------
    str
        ìƒì„±ëœ ì„¤ëª….
    """
    # í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±
    bullet_lines = []
    for feat in top_features:
        arrow = "â†‘" if feat["direction"] == "increase" else "â†“"
        bullet_lines.append(f"- {feat['feature']} : ì˜í–¥ ë°©í–¥ {arrow} (ì¤‘ìš”ë„ {feat['contribution']:.4f})")
    prompt = (
        f"ë‹¹ì‹ ì˜ ìš°ìš¸ì¦ ìœ„í—˜ë„ ì˜ˆì¸¡ ê²°ê³¼ëŠ” {probability * 100:.1f}% ì…ë‹ˆë‹¤.\n"
        f"SHAP ë¶„ì„ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ìš”ì¸ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤:\n"
        + "\n".join(bullet_lines)
        + "\n\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê±´ê°• ì „ë¬¸ê°€ì˜ ì‹œê°ì—ì„œ ê°„ë‹¨í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”. 4-6ë¬¸ì¥ ì´ë‚´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”."
    )
    # LLM ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‹¤í–‰
    if generator is not None:
        try:
            output = generator(
                prompt,
                max_length=len(prompt.split()) + 60,
                num_return_sequences=1,
            )
            generated = output[0]["generated_text"]
            return generated
        except Exception:
            pass
    # ê·œì¹™ ê¸°ë°˜ fallback
    lines: List[str] = []
    lines.append(f"ì˜ˆì¸¡ëœ ìš°ìš¸ì¦ ìœ„í—˜ë„ëŠ” {probability * 100:.1f}%ì…ë‹ˆë‹¤.")
    for feat in top_features:
        kor_direction = "ì¦ê°€" if feat["direction"] == "increase" else "ê°ì†Œ"
        lines.append(f"'{feat['feature']}' ì„­ì·¨ê°€ {kor_direction} ë°©í–¥ìœ¼ë¡œ ìš°ìš¸ì¦ ìœ„í—˜ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.")
    lines.append("ê· í˜• ì¡íŒ ì‹ë‹¨ê³¼ ì ì ˆí•œ ì˜ì–‘ ì„­ì·¨ëŠ” ì •ì‹  ê±´ê°•ì— ê¸ì •ì ì¸ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    return "\n".join(lines)


def respond_chat(user_query: str, generator: Optional[callable] = None) -> str:
    """
    ì±—ë´‡ ì§ˆë¬¸ì— ì‘ë‹µì„ ìƒì„±í•œë‹¤. transformers LLMì´ ìˆìœ¼ë©´ ì´ë¥¼ ì´ìš©í•˜ê³ ,
    ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë‹µë³€ì„ ì œê³µí•œë‹¤.

    Parameters
    ----------
    user_query : str
        ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸.
    generator : Optional[callable], optional
        text-generation pipeline. Noneì´ë©´ ê·œì¹™ ê¸°ë°˜ ì„¤ëª…ì„ ì‚¬ìš©í•œë‹¤.

    Returns
    -------
    str
        ì±—ë´‡ ì‘ë‹µ.
    """
    # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: ì˜ë£Œ ìë¬¸ì´ ì•„ë‹˜ì„ ê°•ì¡°
    base_prompt = (
        "ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê±´ê°• ì •ë³´ ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ë‹¨í•˜ê³  ê¸ì •ì ì¸ ì–¸ì–´ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.\n"
        "ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¸ ë‚´ìš©ì´ ì •ì‹  ê±´ê°•ì— ê´€í•œ ê²ƒì´ë”ë¼ë„, ì •í™•í•œ ì§„ë‹¨ì´ë‚˜ ì¹˜ë£Œë¥¼ ëŒ€ì‹ í•  ìˆ˜ ì—†ìœ¼ë©° ì „ë¬¸ê°€ì™€ ìƒë‹´í•  ê²ƒì„ í•­ìƒ ê¶Œì¥í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        f"ì‚¬ìš©ì: {user_query}\n"
        "ì±—ë´‡:\n"
    )
    if generator is not None:
        try:
            output = generator(base_prompt, max_length=len(base_prompt.split()) + 60, num_return_sequences=1)
            reply = output[0]["generated_text"].split("ì±—ë´‡:")[-1].strip()
            return reply
        except Exception:
            pass
    # ê·œì¹™ ê¸°ë°˜ ë‹µë³€: ì§ˆë¬¸ ë‚´ìš©ì„ ë°˜ë³µí•˜ê³  ì „ë¬¸ê°€ ìƒë‹´ì„ ê¶Œìœ 
    return (
        f"ì§ˆë¬¸í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. '{user_query}'ì— ëŒ€í•´ ì•Œì•„ë³´ë ¤ëŠ” ë‹¹ì‹ ì˜ ë…¸ë ¥ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n"
        "ì €ëŠ” ì •í™•í•œ ì§„ë‹¨ì„ ì œê³µí•  ìˆ˜ ì—†ì§€ë§Œ, ê· í˜• ì¡íŒ ì‹ì‚¬ì™€ ê¾¸ì¤€í•œ ì‹ ì²´ í™œë™ì´ ì •ì‹  ê±´ê°•ì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
        "ë” ìì„¸í•œ ì •ë³´ë‚˜ ìƒë‹´ì´ í•„ìš”í•˜ë‹¤ë©´ ì •ì‹ ê±´ê°• ì „ë¬¸ê°€ì—ê²Œ ë¬¸ì˜í•´ ë³´ì„¸ìš”."
    )


def main() -> None:
    """
    Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë©”ì¸ í•¨ìˆ˜. ê¸°ëŠ¥:
    1. ëª¨ë¸ ë¡œë”© ë° ìºì‹œ
    2. ì‚¬ìš©ì ì…ë ¥ í¼ í‘œì‹œ
    3. ì˜ˆì¸¡ê³¼ ê¸°ì—¬ë„ ê³„ì‚°, ì„¤ëª… ìƒì„±
    4. ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
    """
    st.set_page_config(page_title="ìš°ìš¸ì¦ ìœ„í—˜ë„ ì˜ˆì¸¡", page_icon="ğŸ§ ", layout="centered")
    st.title("ê°œì¸ ë§ì¶¤í˜• ìš°ìš¸ì¦ ìœ„í—˜ë„ ì˜ˆì¸¡")
    disclaimer()
    # ëª¨ë¸ ë¡œë“œ
    model = load_model()
    if model is None:
        st.stop()

    # íŠ¹ì„± ì´ë¦„: scikitâ€‘learn ëª¨ë¸ì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜ ì‚¬ìš©ìì—ê²Œ ì…ë ¥ ë°›ìŒ
    if hasattr(model, "feature_names_in_"):
        feature_names: List[str] = [str(f) for f in model.feature_names_in_]
    else:
        feature_input = st.text_input("ëª¨ë¸ì— ì‚¬ìš©ëœ íŠ¹ì„± ì´ë¦„ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”.")
        if not feature_input:
            st.info("ëª¨ë¸ì— íŠ¹ì„± ì´ë¦„ì„ ì œê³µí•˜ì§€ ì•Šìœ¼ë©´ ì…ë ¥ í¼ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()
        feature_names = [f.strip() for f in feature_input.split(",") if f.strip()]
        if not feature_names:
            st.error("ì˜¬ë°”ë¥¸ íŠ¹ì„± ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”.")
            st.stop()

    # ì‚¬ìš©ì ì…ë ¥: number_input ìœ¼ë¡œ êµ¬ì„±
    st.subheader("ì˜ì–‘ì†Œ ì„­ì·¨ëŸ‰ì„ ì…ë ¥í•˜ì„¸ìš”")
    user_values: Dict[str, float] = {}
    cols = st.columns(2)
    for i, name in enumerate(feature_names):
        with cols[i % 2]:
            user_values[name] = st.number_input(name, value=0.0, step=0.1, format="%.2f")

    # ì˜ˆì¸¡ ì‹¤í–‰ ë²„íŠ¼
    if st.button("ìš°ìš¸ì¦ ìœ„í—˜ë„ ì˜ˆì¸¡í•˜ê¸°"):
        X_input = pd.DataFrame([user_values], columns=feature_names)
        # ëª¨ë¸ì—ì„œ í™•ë¥  ì˜ˆì¸¡
        try:
            proba = model.predict_proba(X_input)[0][1]
        except Exception as e:
            st.error(f"ëª¨ë¸ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            proba = 0.0
        # ê¸°ì—¬ë„ ê³„ì‚°
        contributions = compute_contributions(model, X_input)
        top_feats = get_top_features(contributions, feature_names)
        # LLM ë¡œë”©
        generator = load_text_generator()
        # ì„¤ëª… ìƒì„±
        explanation = build_explanation(proba, top_feats, generator=generator)
        # ê²°ê³¼ í‘œì‹œ
        st.markdown("---")
        st.subheader("ì˜ˆì¸¡ ê²°ê³¼")
        st.metric(label="ìš°ìš¸ì¦ ìœ„í—˜ë„", value=f"{proba*100:.1f}%")
        st.subheader("ì£¼ìš” ì˜í–¥ ìš”ì¸")
        for feat in top_feats:
            arrow = "â†‘" if feat["direction"] == "increase" else "â†“"
            st.write(f"{feat['feature']} : {arrow} (ì¤‘ìš”ë„ {feat['contribution']:.4f})")
        st.subheader("ë§ì¶¤í˜• ì„¤ëª…")
        st.write(explanation)

    # ì±—ë´‡ ì˜ì—­
    st.markdown("---")
    st.subheader("ì±—ë´‡ì—ê²Œ ì§ˆë¬¸í•˜ê¸°")
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    # ì´ì „ ëŒ€í™” í‘œì‹œ
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])
    # ì…ë ¥ ìƒì
    user_question = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.")
    if user_question:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": user_question})
        st.chat_message("user").write(user_question)
        # ì±—ë´‡ ì‘ë‹µ ìƒì„±
        generator = load_text_generator()
        answer = respond_chat(user_question, generator=generator)
        # ì‘ë‹µ ì €ì¥ ë° í‘œì‹œ
        st.session_state.messages.append({"role": "assistant", "content": answer})
        st.chat_message("assistant").write(answer)


if __name__ == "__main__":
    main()