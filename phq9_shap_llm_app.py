from __future__ import annotations

import json
import os
from typing import Dict, List, Optional

import numpy as np
import pandas as pd

import streamlit as st

try:
    # transformersëŠ” ì„ íƒì ì´ë©° ì¸í„°ë„· ì—°ê²°ì´ ì—†ìœ¼ë©´ ë¡œë“œì— ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # type: ignore
    _transformers_available = True
except Exception:
    _transformers_available = False


def disclaimer() -> None:
    """ì‚¬ìš©ìì—ê²Œ ì¤‘ìš”í•œ ë©´ì±…ë¬¸êµ¬ë¥¼ ë³´ì—¬ì¤€ë‹¤."""
    st.warning(
        """ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ê±´ê°• ê´€ë ¨ ì •ë³´ë¥¼ ì°¸ê³ ìš©ìœ¼ë¡œ ì œê³µí•˜ë©°,
        ì „ë¬¸ì ì¸ ì§„ë‹¨ì´ë‚˜ ì¹˜ë£Œë¥¼ ëŒ€ì‹ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìš°ìš¸ì¦ ìœ„í—˜ë„
        ì˜ˆì¸¡ ê²°ê³¼ëŠ” ì°¸ê³  ìš©ë„ë¡œë§Œ ì‚¬ìš©í•´ì•¼ í•˜ë©°, ìì‹ ì˜ ì •ì‹ ê±´ê°•ì—
        ê´€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì˜ë£Œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.""",
        icon="âš ï¸",
    )


@st.cache_resource(show_spinner=False)
def load_model(model_path: str = "phq9_nutrition_model.pkl"):
    """
    Pickle íŒŒì¼ë¡œ ì €ì¥ëœ scikitâ€‘learn ëª¨ë¸ì„ ë¡œë“œí•œë‹¤.

    Parameters
    ----------
    model_path : str, optional
        ëª¨ë¸ íŒŒì¼ ê²½ë¡œ. ê¸°ë³¸ê°’ì€ `phq9_nutrition_model.pkl`ì´ë‹¤.

    Returns
    -------
    object
        joblibìœ¼ë¡œ ë¡œë“œëœ ëª¨ë¸ ê°ì²´. íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¡œë”©ì— ì‹¤íŒ¨í•˜ë©´
        ``None``ì„ ë°˜í™˜í•œë‹¤.
    """
    if not os.path.exists(model_path):
        st.error(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return None
    try:
        import joblib  # ë¡œì»¬ì— ì¡´ì¬í•˜ëŠ” ê²½ëŸ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬
    except ImportError:
        st.error("joblib ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ì— joblibì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    try:
        return joblib.load(model_path)
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None


@st.cache_resource(show_spinner=False)
def load_text_generator(model_name: str = "sshleifer/tiny-gpt2", device: int = -1):
    """
    ê²½ëŸ‰í™”ëœ í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ì„ ë¡œë“œí•œë‹¤. ì¸í„°ë„·ì´ ì°¨ë‹¨ë˜ì–´ ìˆê±°ë‚˜
    transformersê°€ ì—†ëŠ” ê²½ìš° ``None``ì„ ë°˜í™˜í•œë‹¤.

    Parameters
    ----------
    model_name : str, optional
        HuggingFace ëª¨ë¸ ì´ë¦„. ê¸°ë³¸ê°’ì€ ì‘ì€ GPTâ€‘2 ëª¨ë¸ì´ë‹¤.
    device : int, optional
        -1ì€ CPUë¥¼ ì˜ë¯¸í•œë‹¤. í™˜ê²½ì— GPUê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì¥ì¹˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆë‹¤.

    Returns
    -------
    Optional[callable]
        transformers.pipeline ê°ì²´ ë˜ëŠ” ``None``
    """
    if not _transformers_available:
        return None
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        gen = pipeline("text-generation", model=model, tokenizer=tokenizer, device=device)
        return gen
    except Exception:
        # ë¡œë”© ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
        return None


def compute_contributions(model: object, X: pd.DataFrame) -> np.ndarray:
    """
    SHAP ì—†ì´ ê° íŠ¹ì„±ì˜ ê¸°ì—¬ë„ë¥¼ ê·¼ì‚¬í•˜ì—¬ ë°˜í™˜í•œë‹¤. ëª¨ë¸ì— ë”°ë¼
    ``coef_``(ì„ í˜• ëª¨ë¸) ë˜ëŠ” ``feature_importances_``(íŠ¸ë¦¬ ëª¨ë¸)ë¥¼ ì‚¬ìš©í•œë‹¤.

    Parameters
    ----------
    model : object
        scikitâ€‘learn ë¶„ë¥˜ ëª¨ë¸.
    X : pandas.DataFrame
        ë‹¨ì¼ ìƒ˜í”Œì„ í¬í•¨í•˜ëŠ” DataFrame.

    Returns
    -------
    numpy.ndarray
        íŠ¹ì„±ë³„ ê¸°ì—¬ë„ ë²¡í„°. ê¸¸ì´ëŠ” íŠ¹ì„± ê°œìˆ˜ì™€ ê°™ë‹¤.
    """
    n_features = X.shape[1]
    # ê¸°ë³¸ê°’ì€ 0 ê¸°ì—¬ë„
    contributions = np.zeros(n_features)
    try:
        if hasattr(model, "coef_"):
            # ì„ í˜• ëª¨ë¸: coef_ í¬ê¸° (n_classes, n_features)
            # ì–‘ì„± í´ë˜ìŠ¤(ì¸ë±ìŠ¤ 0)ì„ ì‚¬ìš©í•˜ì—¬ ê°’ ê³±ì…ˆ
            coef = model.coef_[0]
            contributions = coef * X.iloc[0].values
        elif hasattr(model, "feature_importances_"):
            importances = model.feature_importances_
            contributions = importances * X.iloc[0].values
    except Exception:
        # ì˜ˆì™¸ ë°œìƒ ì‹œ 0 ê¸°ì—¬ë„ë¥¼ ìœ ì§€
        pass
    return contributions


def get_top_features(
    contributions: np.ndarray, feature_names: List[str], top_n: int = 5
) -> List[Dict[str, object]]:
    """
    ê¸°ì—¬ë„ ë²¡í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ì˜í–¥ë ¥ì´ í° íŠ¹ì„± ëª©ë¡ì„ ë°˜í™˜í•œë‹¤.

    Parameters
    ----------
    contributions : numpy.ndarray
        ê° íŠ¹ì„±ì˜ ê¸°ì—¬ë„ ë²¡í„°.
    feature_names : List[str]
        íŠ¹ì„± ì´ë¦„ ëª©ë¡.
    top_n : int, optional
        ë°˜í™˜í•  ìƒìœ„ íŠ¹ì„± ê°œìˆ˜. ê¸°ë³¸ê°’ì€ 5ì´ë‹¤.

    Returns
    -------
    List[Dict[str, object]]
        ``feature``, ``contribution``, ``direction`` í‚¤ë¥¼ ê°€ì§„ dict ëª©ë¡.
    """
    # ì ˆëŒ€ê°’ ê¸°ì¤€ ì •ë ¬
    indices = np.argsort(np.abs(contributions))[::-1][: min(top_n, len(contributions))]
    results: List[Dict[str, object]] = []
    for idx in indices:
        val = float(contributions[idx])
        direction = "increase" if val > 0 else "decrease"
        results.append({"feature": feature_names[idx], "contribution": abs(val), "direction": direction})
    return results


def build_explanation(
    probability: float,
    top_features: List[Dict[str, object]],
    generator: Optional[callable] = None,
) -> str:
    """
    ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì£¼ìš” íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ ì„¤ëª…ë¬¸ì„ ìƒì„±í•œë‹¤.
    ê°€ëŠ¥í•œ ê²½ìš° ê²½ëŸ‰ LLMì„ ì‚¬ìš©í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•œë‹¤.

    Parameters
    ----------
    probability : float
        ê¸ì • í´ë˜ìŠ¤(ìš°ìš¸ì¦ ìœ„í—˜) í™•ë¥ .
    top_features : List[Dict[str, object]]
        ê°€ì¥ ì˜í–¥ë ¥ì´ í° íŠ¹ì„± ëª©ë¡.
    generator : Optional[callable], optional
        transformers.pipeline ê°ì²´. Noneì´ë©´ ê·œì¹™ ê¸°ë°˜ ì„¤ëª…ì„ ì‚¬ìš©í•œë‹¤.

    Returns
    -------
    str
        ìƒì„±ëœ ì„¤ëª….
    """
    # í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±
    bullet_lines = []
    for feat in top_features:
        arrow = "â†‘" if feat["direction"] == "increase" else "â†“"
        bullet_lines.append(f"- {feat['feature']} : ì˜í–¥ ë°©í–¥ {arrow} (ì¤‘ìš”ë„ {feat['contribution']:.4f})")
    prompt = (
        f"ë‹¹ì‹ ì˜ ìš°ìš¸ì¦ ìœ„í—˜ë„ ì˜ˆì¸¡ ê²°ê³¼ëŠ” {probability * 100:.1f}% ì…ë‹ˆë‹¤.\n"
        f"SHAP ë¶„ì„ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ìš”ì¸ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤:\n"
        + "\n".join(bullet_lines)
        + "\n\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê±´ê°• ì „ë¬¸ê°€ì˜ ì‹œê°ì—ì„œ ê°„ë‹¨í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”. 4-6ë¬¸ì¥ ì´ë‚´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”."
    )
    # LLM ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‹¤í–‰
    if generator is not None:
        try:
            output = generator(
                prompt,
                max_length=len(prompt.split()) + 60,
                num_return_sequences=1,
            )
            generated = output[0]["generated_text"]
            return generated
        except Exception:
            pass
    # ê·œì¹™ ê¸°ë°˜ fallback
    lines: List[str] = []
    lines.append(f"ì˜ˆì¸¡ëœ ìš°ìš¸ì¦ ìœ„í—˜ë„ëŠ” {probability * 100:.1f}%ì…ë‹ˆë‹¤.")
    for feat in top_features:
        kor_direction = "ì¦ê°€" if feat["direction"] == "increase" else "ê°ì†Œ"
        # ë³€ìˆ˜ ê°’ì˜ ë³€í™” ë°©í–¥ì— ë”°ë¼ ìœ„í—˜ë„ ì˜í–¥ì„ ì„¤ëª…
        lines.append(f"'{feat['feature']}' ê°’ì´ {kor_direction} ë°©í–¥ìœ¼ë¡œ ìš°ìš¸ì¦ ìœ„í—˜ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.")
    lines.append("ì •ê¸°ì ì¸ ìš´ë™ê³¼ ê· í˜• ì¡íŒ ìƒí™œ ìŠµê´€ì´ ì •ì‹  ê±´ê°• ìœ ì§€ì— ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    return "\n".join(lines)


def respond_chat(user_query: str, generator: Optional[callable] = None) -> str:
    """
    ì±—ë´‡ ì§ˆë¬¸ì— ì‘ë‹µì„ ìƒì„±í•œë‹¤. transformers LLMì´ ìˆìœ¼ë©´ ì´ë¥¼ ì´ìš©í•˜ê³ ,
    ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë‹µë³€ì„ ì œê³µí•œë‹¤.

    Parameters
    ----------
    user_query : str
        ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸.
    generator : Optional[callable], optional
        text-generation pipeline. Noneì´ë©´ ê·œì¹™ ê¸°ë°˜ ì„¤ëª…ì„ ì‚¬ìš©í•œë‹¤.

    Returns
    -------
    str
        ì±—ë´‡ ì‘ë‹µ.
    """
    # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸: ì˜ë£Œ ìë¬¸ì´ ì•„ë‹˜ì„ ê°•ì¡°
    base_prompt = (
        "ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê±´ê°• ì •ë³´ ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ë‹¨í•˜ê³  ê¸ì •ì ì¸ ì–¸ì–´ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.\n"
        "ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¸ ë‚´ìš©ì´ ì •ì‹  ê±´ê°•ì— ê´€í•œ ê²ƒì´ë”ë¼ë„, ì •í™•í•œ ì§„ë‹¨ì´ë‚˜ ì¹˜ë£Œë¥¼ ëŒ€ì‹ í•  ìˆ˜ ì—†ìœ¼ë©° ì „ë¬¸ê°€ì™€ ìƒë‹´í•  ê²ƒì„ í•­ìƒ ê¶Œì¥í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        f"ì‚¬ìš©ì: {user_query}\n"
        "ì±—ë´‡:\n"
    )
    if generator is not None:
        try:
            output = generator(base_prompt, max_length=len(base_prompt.split()) + 60, num_return_sequences=1)
            reply = output[0]["generated_text"].split("ì±—ë´‡:")[-1].strip()
            return reply
        except Exception:
            pass
    # ê·œì¹™ ê¸°ë°˜ ë‹µë³€: ì§ˆë¬¸ ë‚´ìš©ì„ ë°˜ë³µí•˜ê³  ì „ë¬¸ê°€ ìƒë‹´ì„ ê¶Œìœ 
    return (
        f"ì§ˆë¬¸í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. '{user_query}'ì— ëŒ€í•´ ì•Œì•„ë³´ë ¤ëŠ” ë‹¹ì‹ ì˜ ë…¸ë ¥ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n"
        "ì €ëŠ” ì •í™•í•œ ì§„ë‹¨ì„ ì œê³µí•  ìˆ˜ ì—†ì§€ë§Œ, ê· í˜• ì¡íŒ ì‹ì‚¬ì™€ ê¾¸ì¤€í•œ ì‹ ì²´ í™œë™ì´ ì •ì‹  ê±´ê°•ì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
        "ë” ìì„¸í•œ ì •ë³´ë‚˜ ìƒë‹´ì´ í•„ìš”í•˜ë‹¤ë©´ ì •ì‹ ê±´ê°• ì „ë¬¸ê°€ì—ê²Œ ë¬¸ì˜í•´ ë³´ì„¸ìš”."
    )


def main() -> None:
    """
    Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë©”ì¸ í•¨ìˆ˜.

    * ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤.
    * ë¯¸ë¦¬ ì •ì˜í•œ ë‹¤ì–‘í•œ ì¸êµ¬ í†µê³„, ê±´ê°•, ì‹ìŠµê´€ ë³€ìˆ˜ì— ëŒ€í•´ ì…ë ¥ í¼ì„ ì œê³µí•©ë‹ˆë‹¤.
    * ì…ë ¥ëœ ê°’ìœ¼ë¡œ ìš°ìš¸ì¦ ìœ„í—˜ë„ë¥¼ ì˜ˆì¸¡í•˜ê³ , ê¸°ì—¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ì£¼ìš” ìš”ì¸ì„ í‘œì‹œí•©ë‹ˆë‹¤.
    * ì„¤ëª…ì„ LLM ë˜ëŠ” ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    * ê°„ë‹¨í•œ ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ì¶”ê°€ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
    """
    st.set_page_config(page_title="ìš°ìš¸ì¦ ìœ„í—˜ë„ ì˜ˆì¸¡", page_icon="ğŸ§ ", layout="centered")
    st.title("ê°œì¸ ë§ì¶¤í˜• ìš°ìš¸ì¦ ìœ„í—˜ë„ ì˜ˆì¸¡")
    disclaimer()

    # ëª¨ë¸ ë¡œë“œ
    model = load_model()
    if model is None:
        st.stop()

    # ì…ë ¥ ë³€ìˆ˜ ì •ì˜: ë³€ìˆ˜ëª…ê³¼ ë¼ë²¨, íƒ€ì…, ì˜µì…˜ ì§€ì •
    feature_definitions = [
        ("sex", {"label": "ì„±ë³„", "type": "select", "options": {1: "ë‚¨ì", 2: "ì—¬ì"}}),
        ("age", {"label": "í˜„ì¬ ë‚˜ì´", "type": "number"}),
        ("individual_income", {"label": "ê°œì¸ì†Œë“", "type": "select", "options": {1: "ë§¤ìš° ë‚®ìŒ", 2: "ë‚®ìŒ", 3: "ë†’ìŒ", 4: "ë§¤ìš° ë†’ìŒ"}}),
        ("household_income", {"label": "ê°€êµ¬ì†Œë“", "type": "select", "options": {1: "ë§¤ìš° ë‚®ìŒ", 2: "ë‚®ìŒ", 3: "ë†’ìŒ", 4: "ë§¤ìš° ë†’ìŒ"}}),
        ("education_level", {"label": "í•™ë ¥", "type": "select", "options": {1: "ì´ˆë“±í•™êµ ì´í•˜", 2: "ì¤‘í•™êµ ì¡¸ì—…", 3: "ê³ ë“±í•™êµ ì¡¸ì—…", 4: "ëŒ€í•™êµ ì´ìƒ"}}),
        ("occupation", {"label": "ì§ì—… ì—¬ë¶€", "type": "select", "options": {1: "ì§ì—… ìˆìŒ", 0: "ì§ì—… ì—†ìŒ"}}),
        ("number_of_household_member", {"label": "ë…ê±° ì—¬ë¶€", "type": "select", "options": {1: "ë…ê±°", 2: "ë™ê±°"}}),
        ("house_status", {"label": "ì£¼íƒ ì†Œìœ  ì—¬ë¶€", "type": "select", "options": {1: "ì†Œìœ ", 0: "ë¯¸ì†Œìœ "}}),
        ("marital_statues", {"label": "ê²°í˜¼ ì—¬ë¶€", "type": "select", "options": {1: "ê¸°í˜¼", 0: "ë¯¸í˜¼"}}),
        ("subjective_health_status", {"label": "ì£¼ê´€ì  ê±´ê°•ìƒíƒœ", "type": "select", "options": {1: "ë‚˜ì¨", 2: "ë³´í†µ", 3: "ì¢‹ìŒ"}}),
        ("unmet_medical_care", {"label": "ì˜ë£Œ ì´ìš© ì—¬ë¶€", "type": "select", "options": {1: "ì¹˜ë£Œ ë°›ì§€ ëª»í•¨", 0: "ì¹˜ë£Œ ë°›ìŒ"}}),
        ("labor_hour", {"label": "ì£¼ê°„ ê·¼ë¡œì‹œê°„", "type": "number"}),
        ("smoking", {"label": "í¡ì—° ì—¬ë¶€", "type": "select", "options": {1: "í¡ì—°ì", 0: "ë¹„í¡ì—°ì"}}),
        ("drinking", {"label": "ìŒì£¼ ì—¬ë¶€", "type": "select", "options": {1: "ìŒì£¼ì", 0: "ë¹„ìŒì£¼ì"}}),
        ("stress", {"label": "ìŠ¤íŠ¸ë ˆìŠ¤ ì •ë„", "type": "select", "options": {1: "ìŠ¤íŠ¸ë ˆìŠ¤ ì—†ìŒ", 2: "ìŠ¤íŠ¸ë ˆìŠ¤ ë‚®ìŒ", 3: "ìŠ¤íŠ¸ë ˆìŠ¤ ë†’ìŒ", 4: "ìŠ¤íŠ¸ë ˆìŠ¤ ë§¤ìš° ë†’ìŒ"}}),
        ("hpa_work", {"label": "ì¼ë¡œ ì¸í•œ ê³ ê°•ë„ ì‹ ì²´í™œë™", "type": "select", "options": {1: "ì˜ˆ", 0: "ì•„ë‹ˆì˜¤"}}),
        ("mpa_work", {"label": "ì¼ë¡œ ì¸í•œ ì¤‘ë“±ë„ ì‹ ì²´í™œë™", "type": "select", "options": {1: "ì˜ˆ", 0: "ì•„ë‹ˆì˜¤"}}),
        ("hpa_leisure", {"label": "ì—¬ê°€ë¡œ ê³ ê°•ë„ ì‹ ì²´í™œë™", "type": "select", "options": {1: "ì˜ˆ", 0: "ì•„ë‹ˆì˜¤"}}),
        ("mpa_leisure", {"label": "ì—¬ê°€ë¡œ ì¤‘ë“±ë„ ì‹ ì²´í™œë™", "type": "select", "options": {1: "ì˜ˆ", 0: "ì•„ë‹ˆì˜¤"}}),
        ("walk", {"label": "ê±·ê¸° ì—¬ë¶€", "type": "select", "options": {1: "ì˜ˆ", 0: "ì•„ë‹ˆì˜¤"}}),
        ("sedantary_hour", {"label": "í•˜ë£¨ í‰ê·  ì•‰ì•„ìˆëŠ” ì‹œê°„", "type": "number"}),
        ("body_mass_index", {"label": "ì²´ì§ˆëŸ‰ì§€ìˆ˜", "type": "number"}),
        ("food_intake", {"label": "ì‹í’ˆ ì„­ì·¨ëŸ‰", "type": "number"}),
        ("calorie_intake", {"label": "ì¹¼ë¡œë¦¬ ì„­ì·¨ëŸ‰", "type": "number"}),
        ("weter_intake", {"label": "ë¬¼ ì„­ì·¨ëŸ‰", "type": "number"}),
        ("protein", {"label": "ë‹¨ë°±ì§ˆ ì„­ì·¨ëŸ‰", "type": "number"}),
        ("saturated_fatty_acid", {"label": "í¬í™”ì§€ë°©ì‚° ì„­ì·¨ëŸ‰", "type": "number"}),
        ("mono_unsaturated_fatty_acid", {"label": "ë‹¨ì¼ë¶ˆí¬í™”ì§€ë°©ì‚° ì„­ì·¨ëŸ‰", "type": "number"}),
        ("n3_fatty_acid", {"label": "n3 ì§€ë°©ì‚° ì„­ì·¨ëŸ‰", "type": "number"}),
        ("n6_fatty_acid", {"label": "n6 ì§€ë°©ì‚° ì„­ì·¨ëŸ‰", "type": "number"}),
        ("cholesterol", {"label": "ì½œë ˆìŠ¤í…Œë¡¤ ì„­ì·¨ëŸ‰", "type": "number"}),
        ("carbohydrate", {"label": "íƒ„ìˆ˜í™”ë¬¼ ì„­ì·¨ëŸ‰", "type": "number"}),
        ("dietary_fiber", {"label": "ì‹ì´ì„¬ìœ  ì„­ì·¨ëŸ‰", "type": "number"}),
        ("calcium", {"label": "ì¹¼ìŠ˜ ì„­ì·¨ëŸ‰", "type": "number"}),
        ("phosphorus", {"label": "ì¸ ì„­ì·¨ëŸ‰", "type": "number"}),
        ("iron", {"label": "ì² ë¶„ ì„­ì·¨ëŸ‰", "type": "number"}),
        ("soudim", {"label": "ë‚˜íŠ¸ë¥¨ ì„­ì·¨ëŸ‰", "type": "number"}),
        ("potassium", {"label": "ì¹¼ë¥¨ ì„­ì·¨ëŸ‰", "type": "number"}),
        ("betacarotine", {"label": "ë² íƒ€ì¹´ë¡œí‹´ ì„­ì·¨ëŸ‰", "type": "number"}),
        ("retinol", {"label": "ë ˆí‹°ë†€ ì„­ì·¨ëŸ‰", "type": "number"}),
        ("vitamin_b1", {"label": "ë¹„íƒ€ë¯¼ B1 ì„­ì·¨ëŸ‰", "type": "number"}),
        ("vitamin_b2", {"label": "ë¹„íƒ€ë¯¼ B2 ì„­ì·¨ëŸ‰", "type": "number"}),
        ("vitamin_b3", {"label": "ë¹„íƒ€ë¯¼ B3 ì„­ì·¨ëŸ‰", "type": "number"}),
        ("vitamin_c", {"label": "ë¹„íƒ€ë¯¼ C ì„­ì·¨ëŸ‰", "type": "number"}),
        ("cardiovascular_disease", {"label": "ì‹¬í˜ˆê´€ ì§ˆí™˜ ì—¬ë¶€", "type": "select", "options": {1: "ìˆìŒ", 0: "ì—†ìŒ"}}),
        ("arthritis_disease", {"label": "ê´€ì ˆì—¼ ì—¬ë¶€", "type": "select", "options": {1: "ìˆìŒ", 0: "ì—†ìŒ"}}),
        ("pulmonary_disease", {"label": "í˜¸í¡ê¸°ê³„ ì§ˆí™˜ ì—¬ë¶€", "type": "select", "options": {1: "ìˆìŒ", 0: "ì—†ìŒ"}}),
        ("liver_disease", {"label": "ê°„ ì§ˆí™˜ ì—¬ë¶€", "type": "select", "options": {1: "ìˆìŒ", 0: "ì—†ìŒ"}}),
        ("thyroid_disease", {"label": "ê°‘ìƒì„  ì§ˆí™˜ ì—¬ë¶€", "type": "select", "options": {1: "ìˆìŒ", 0: "ì—†ìŒ"}}),
        ("t2_diabetes_mellitus", {"label": "ì œ2í˜• ë‹¹ë‡¨ë³‘ ì—¬ë¶€", "type": "select", "options": {1: "ìˆìŒ", 0: "ì—†ìŒ"}}),
        ("atopic_dermatitis", {"label": "ì•„í† í”¼ í”¼ë¶€ì—¼ ì—¬ë¶€", "type": "select", "options": {1: "ìˆìŒ", 0: "ì—†ìŒ"}}),
        ("allergic_rhinitis", {"label": "ì•Œë ˆë¥´ê¸°ì„± ë¹„ì—¼ ì—¬ë¶€", "type": "select", "options": {1: "ìˆìŒ", 0: "ì—†ìŒ"}}),
        ("renal_disease", {"label": "ì‹ ì¥ ì§ˆí™˜ ì—¬ë¶€", "type": "select", "options": {1: "ìˆìŒ", 0: "ì—†ìŒ"}}),
        ("cancer", {"label": "ì•” ì—¬ë¶€", "type": "select", "options": {1: "ìˆìŒ", 0: "ì—†ìŒ"}}),
    ]

    feature_names: List[str] = [name for name, _ in feature_definitions]
    st.subheader("ì…ë ¥ íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí•˜ì„¸ìš”")
    user_values: Dict[str, float] = {}
    cols = st.columns(2)
    # ê° ì…ë ¥ í•„ë“œë¥¼ ìƒì„±
    for i, (name, info) in enumerate(feature_definitions):
        with cols[i % 2]:
            if info["type"] == "select":
                display_options = list(info["options"].values())
                selected_display = st.selectbox(info["label"], display_options, key=name)
                for code, disp in info["options"].items():
                    if disp == selected_display:
                        user_values[name] = code
                        break
            else:
                value = st.number_input(info["label"], value=0.0, step=0.1, key=name)
                user_values[name] = float(value)

    # ì˜ˆì¸¡ ì‹¤í–‰
    if st.button("ìš°ìš¸ì¦ ìœ„í—˜ë„ ì˜ˆì¸¡í•˜ê¸°"):
        X_input = pd.DataFrame([user_values], columns=feature_names)
        try:
            proba = model.predict_proba(X_input)[0][1]
        except Exception as e:
            st.error(f"ëª¨ë¸ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            proba = 0.0
        contributions = compute_contributions(model, X_input)
        top_feats = get_top_features(contributions, feature_names)
        generator = load_text_generator()
        explanation = build_explanation(proba, top_feats, generator=generator)
        st.markdown("---")
        st.subheader("ì˜ˆì¸¡ ê²°ê³¼")
        st.metric(label="ìš°ìš¸ì¦ ìœ„í—˜ë„", value=f"{proba*100:.1f}%")
        st.subheader("ì£¼ìš” ì˜í–¥ ìš”ì¸")
        for feat in top_feats:
            arrow = "â†‘" if feat["direction"] == "increase" else "â†“"
            st.write(f"{feat['feature']} : {arrow} (ì¤‘ìš”ë„ {feat['contribution']:.4f})")
        st.subheader("ë§ì¶¤í˜• ì„¤ëª…")
        st.write(explanation)

    # ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
    st.markdown("---")
    st.subheader("ì±—ë´‡ì—ê²Œ ì§ˆë¬¸í•˜ê¸°")
    if "messages" not in st.session_state:
        st.session_state.messages = []
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])
    user_question = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.")
    if user_question:
        st.session_state.messages.append({"role": "user", "content": user_question})
        st.chat_message("user").write(user_question)
        generator = load_text_generator()
        answer = respond_chat(user_question, generator=generator)
        st.session_state.messages.append({"role": "assistant", "content": answer})
        st.chat_message("assistant").write(answer)


if __name__ == "__main__":
    main()
